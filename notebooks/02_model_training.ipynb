{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reading the csv cleaned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/happiness_dataset_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AI model - Let's training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, LassoLars, LassoLarsIC, LassoLarsCV, Ridge, RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear Regression: 0.9852277849790076\n",
      "Accuracy of Lasso: 0.9849907887476183\n",
      "Accuracy of Ridge: 0.9853341629783893\n",
      "Accuracy of RidgeCV: 0.9853341629735776\n",
      "Accuracy of LassoCV: 0.9854045096329477\n",
      "Accuracy of LassoLars: 0.9849907887476183\n",
      "Accuracy of LassoLarsCV: 0.9853518330190502\n",
      "Accuracy of LassoLarsIC: 0.985289612178536\n",
      "Accuracy of XGBRegressor: 0.9966248551540543\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(['happiness_score', 'country'], axis=1)\n",
    "y = df['happiness_score']\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge(),\n",
    "    'RidgeCV': RidgeCV(),\n",
    "    'LassoCV': LassoCV(), \n",
    "    'LassoLars': LassoLars(),\n",
    "    'LassoLarsCV': LassoLarsCV(),\n",
    "    'LassoLarsIC': LassoLarsIC(),\n",
    "    'XGBRegressor': XGBRegressor(), #Hasta ahora la más precisa\n",
    "}\n",
    "\n",
    "def train_model(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    accuracy = r2_score(y_test, y_predict)\n",
    "    return accuracy\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'Accuracy of {model_name}: {train_model(model, X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusiones del Modelo**\n",
    "Después de entrenar varios modelos de regresión lineal y regularización, se obtuvieron las siguientes métricas de precisión (R²):\n",
    "\n",
    "- **Linear Regression**: 0.9852277849790076\n",
    "- **Lasso**: 0.9849907887476183\n",
    "- **Ridge**: 0.9853341629783893\n",
    "- **RidgeCV**: 0.9853341629735776\n",
    "- **LassoCV**: 0.9854045096329477\n",
    "- **LassoLars**: 0.9849907887476183\n",
    "- **LassoLarsCV**: 0.9853518330190502\n",
    "- **LassoLarsIC**: 0.985289612178536\n",
    "- **XGBRegressor**: 0.9966248551540543\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "1. **XGBRegressor** es el modelo que obtuvo la mayor precisión con un R² de 99.66%.\n",
    "\n",
    "2. **LassoCV** también mostró un buen rendimiento con una precisión muy alta, lo que indica que la regularización Lasso es efectiva para este conjunto de datos.\n",
    "\n",
    "3. **Lasso** y **LassoLars** también mostraron un buen rendimiento con una precisión muy cercana a la de LassoCV.\n",
    "\n",
    "4. **Ridge** y **RidgeCV** tuvieron un rendimiento similar, con una precisión ligeramente inferior a la de los modelos Lasso, pero aún así muy alta.\n",
    "\n",
    "5. **Linear Regression** tuvo la precisión más baja entre los modelos probados, aunque la diferencia es mínima.\n",
    "\n",
    "En resumen, todos los modelos probados mostraron una alta precisión, con XGBRegressor destacándose ligeramente sobre los demás. Por lo anterior, **se utilizará el modelo de XGBRegressor** para presentar predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved XGBoost model to ../artifacts/models/model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the XGBoost model again\n",
    "xgboost_model = XGBRegressor(eval_metric='logloss')\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model to disk\n",
    "path = '../artifacts/models/model.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(xgboost_model, f)\n",
    "\n",
    "print(f\"Successfully saved XGBoost model to {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop003-data-and-ai-engineering-OhencQoa-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
